{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88670989-b6d8-46a9-a1cb-b9375a3028b0",
   "metadata": {},
   "source": [
    "# 【双归一化网络】训练DINO\n",
    "\n",
    "## 20230509log\n",
    "* 基于NB01。预计加入tensorboard可视化\n",
    "* 去除了测试部分的代码，使得整体更加简洁\n",
    "\n",
    "## 20230513log 达到较为简洁的程度\n",
    "\n",
    "* 【整理】把数据集、投射层、DINO数据增强放到了共享文件里面（TerrainDataset, Proj_layer, DataAugmentationDINO）\n",
    "- 【理解】理清楚了训练（NB03）和测试（NB05）中使用的数据增强，\n",
    "    - 训练使用DINOAug返回10张图像的list和标签组成的元组（list(tensor1, tensor2……), index）\n",
    "    - 测试使用简单的transform返回一个tensor和标签组成的元组（tensor, index）\n",
    "* 【删除】删除了Xcit的代码，因为并未使用它\n",
    "* 【更改】TerrainDataset。**TerrainDataset中多做一步**，判断每次是增强了多个还是一个，多个的话返回list，一个直接返回tensor\n",
    "\n",
    "## 20230525log \n",
    "\n",
    "* 【描述】【到NB13】改为双结构网络\n",
    "\n",
    "## 20230528log\n",
    "\n",
    "* 【描述】【NB13】加入EEnT支持\n",
    "* 【使用】\n",
    "    * 1 只需要改变 \"--arch\" 参数就可以改变训练时的架构了\n",
    "    * 2 dataset_path变量：确保训练数据集是想要的\n",
    "    * 3 \"--num_ele_slice\"参数为EEnT网络专有的，最好保持不变，在调试的时候再更改，确保后续步骤（NB14到16中），这一切片数量参数都保持一致才能运行\n",
    "\n",
    "## 20230605log\n",
    "\n",
    "* 【特性】加入连续运行Nb15编码存储与NB16匹配\n",
    "    \n",
    "## 20230714log\n",
    "\n",
    "* 【特性】加入Py10中使用slice、per_li信息的兼容\n",
    "\n",
    "## 20230718log\n",
    "\n",
    "* 【特性】加入训练时给全局归一化数据增强的机制（TerrainDataset_BiNorm_Noise_on_g）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b70946-5a49-43a4-acf9-1f1d248915ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Py01shared_code import TerrainDataset, Proj_layer, DataAugmentationDINO, get_args_parser, get_netpath_unique\n",
    "from Py01shared_code import AddGaussianNoise, AddGaussianNoise_Pre_Snr, AddPepperNoise\n",
    "from Py01shared_code import DataAugmentationDINO_ADJ, DataAugmentationDINO_ADJ_V2, DataAugmentationDINO_ADJ_V3\n",
    "from Py06BiNet import MultiCropWrapper_Bi, DINOHead_Bi, TBiNet, TerrainDataset_BiNorm, TerrainDataset_BiNorm_Noise_on_g, get_dataset_minmax, TBiNet_Res\n",
    "from Py07EEnT import ElevationTransformer, EEnT_tiny\n",
    "from Py08EEnT_NO2 import get_dataset_per, ElevationTransformer_NO2, EEnT_NO2_tiny\n",
    "import Py10Bi_model\n",
    "from Py13_SA_FETM import get_bof_voc, get_dataset_var_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c671f44-f036-4fc2-9f7e-1a0ed40b66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from torchvision import transforms as pth_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a423d1-564d-404b-b10a-f66a7e094fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = './Data_geo/20230517【补充多步长切分数据】/low-step-20'\n",
    "dataset_path = \"./Data_geo/深度学习地形数据/train\"\n",
    "\n",
    "args_self = [\n",
    "\"--lr\", \"0.0005\",\n",
    "'--dist_url', 'tcp://localhost:10001' ,\n",
    "\"--num_workers\", \"1\",\n",
    "'--batch_size_per_gpu', '45', # 64正好跑满24G显存(加上了双归一化数据及后不行了) # 对于SA_FETM_bi，50比较保险\n",
    "'--arch', \"GRET_ELE_DB_NO2_bi\", # 一般可选 BiNet_NO1、EEnT_tiny_NO1、EEnT_tiny_NO2, BiNet_Res_NO1, 与Py10中的网络 如 TBiNet_Res_NO2_bi、SA_FETM_bi、EEnT_NO3_AT_bi\n",
    "\"--num_ele_slice\", \"20000\", # 专为EEnT中的高程嵌入使用的参数 \n",
    "\"--epochs\", \"100\"\n",
    "]\n",
    "parser = argparse.ArgumentParser('DINO', parents=[get_args_parser()])\n",
    "args = parser.parse_args(args = args_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6367273c-5807-47cd-a037-3fec6e46d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_bof_voc = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acafaa83-3fae-41d5-8abb-b8db8f2e9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_ADJ = DataAugmentationDINO_ADJ_V3( # 基于最初始版本的DINOAug更改\n",
    "#     args.global_crops_scale,\n",
    "#     args.local_crops_scale,\n",
    "#     args.local_crops_number,\n",
    "# #     USE_Noise_Aug=False, # 默认值为True，也即注释后，为True\n",
    "# #     Use_HFlip_aug=False,\n",
    "# #     Use_VFlip_aug=False,\n",
    "# #     Use_Rotate_aug=False,\n",
    "# #     Rotate_angle = 1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ffd76a6-c6c4-4313-adee-e3032e728ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:803: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform_ADJ = DataAugmentationDINO_ADJ( # 基于最初始版本的DINOAug更改\n",
    "    args.global_crops_scale,\n",
    "    args.local_crops_scale,\n",
    "    args.local_crops_number,\n",
    "#     USE_Noise_Aug=False, # 默认值为True，也即注释后，为True\n",
    "#     Use_HFlip_aug=False,\n",
    "#     Use_VFlip_aug=False,\n",
    ")\n",
    "\n",
    "transform_G = pth_transforms.Compose([\n",
    "    AddPepperNoise(0.9999, p=1.0),\n",
    "    pth_transforms.ToTensor(),\n",
    "#         pth_transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e34cbc7-1634-4906-ad84-c2123fef2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_ADJ = DataAugmentationDINO( # 原始版本的增强，不修改\n",
    "#     args.global_crops_scale,\n",
    "#     args.local_crops_scale,\n",
    "#     args.local_crops_number,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2319853e-248b-4426-9b1f-4e2ae871b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def REA_gating_record(epoch, teacher, _):\n",
    "#     print(args.arch)\n",
    "    if args.arch[0:4] == \"GRET\":\n",
    "        # 如果为0，记录头\n",
    "        if epoch == 0 : \n",
    "            f = open(\"./【输出】NB13_gating_para.csv\", \"w\")\n",
    "            f.write(\"epochs, \")\n",
    "            for i,block in enumerate(teacher.backbone.blocks):\n",
    "                for name,para in block.named_parameters():\n",
    "                    if name == \"gating_param\":\n",
    "                        f.write(str(i+1) + \", \")\n",
    "            f.write(\"\\n\")\n",
    "            f.close()\n",
    "        \n",
    "        # 记录每一行（每一个epoch）\n",
    "        f = open(\"./【输出】NB13_gating_para.csv\", \"a\")\n",
    "        f.write(str(epoch)+\",\" )\n",
    "        for _, block in enumerate(teacher.backbone.blocks):\n",
    "            for name,para in block.named_parameters():\n",
    "            #     print(name)\n",
    "                if name == \"gating_param\":\n",
    "        #             print(\"gating_param = \", torch.sigmoid(para))\n",
    "                    f.write(str(float(torch.sigmoid(para))) + \", \")\n",
    "        f.write(\"\\n\")\n",
    "        f.close()\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d9938a7-e2fc-423d-b2a5-5a19310bb2ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models as torchvision_models\n",
    "from torch.utils.tensorboard import SummaryWriter   \n",
    "\n",
    "import utils\n",
    "import vision_transformer as vits\n",
    "from functools import partial\n",
    "from utils import trunc_normal_\n",
    "\n",
    "\n",
    "# from functools import partial\n",
    "# from torch.utils.data import _utils\n",
    "\n",
    "# from timm.models.vision_transformer import _cfg, Mlp\n",
    "# from timm.models.registry import register_model\n",
    "# from timm.models.layers import DropPath, trunc_normal_, to_2tuple\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "# def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "#     # type: (Tensor, float, float, float, float) -> Tensor\n",
    "#     return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "#定义投影头及其前向传播并初始化其参数\n",
    "class DINOHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):\n",
    "        super().__init__()\n",
    "        nlayers = max(nlayers, 1)\n",
    "        if nlayers == 1:\n",
    "            self.mlp = nn.Linear(in_dim, bottleneck_dim)\n",
    "        else:\n",
    "            layers = [nn.Linear(in_dim, hidden_dim)]\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            for _ in range(nlayers - 2):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                if use_bn:\n",
    "                    layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "                layers.append(nn.GELU())\n",
    "            layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
    "            self.mlp = nn.Sequential(*layers)\n",
    "        self.apply(self._init_weights)\n",
    "        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))\n",
    "        self.last_layer.weight_g.data.fill_(1)\n",
    "        if norm_last_layer:\n",
    "            self.last_layer.weight_g.requires_grad = False\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        x = nn.functional.normalize(x, dim=-1, p=2)\n",
    "        x = self.last_layer(x)\n",
    "        return x\n",
    "\n",
    "class MultiCropWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Perform forward pass separately on each resolution input.\n",
    "    The inputs corresponding to a single resolution are clubbed and single\n",
    "    forward is run on the same resolution inputs. Hence we do several\n",
    "    forward passes = number of different resolutions used. We then\n",
    "    concatenate all the output features and run the head forward on these\n",
    "    concatenated features.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, head):\n",
    "        super(MultiCropWrapper, self).__init__()\n",
    "        # disable layers dedicated to ImageNet labels classification\n",
    "        backbone.fc, backbone.head = nn.Identity(), nn.Identity()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convert to list\n",
    "        if not isinstance(x, list):\n",
    "            x = [x]\n",
    "        idx_crops = torch.cumsum(torch.unique_consecutive(\n",
    "            torch.tensor([inp.shape[-1] for inp in x]),\n",
    "            return_counts=True,\n",
    "        )[1], 0)\n",
    "        start_idx, output = 0, torch.empty(0).to(x[0].device)\n",
    "        for end_idx in idx_crops:\n",
    "            _out = self.backbone(torch.cat(x[start_idx: end_idx]))\n",
    "            # The output is a tuple with XCiT model. See:\n",
    "            # https://github.com/facebookresearch/xcit/blob/master/xcit.py#L404-L405\n",
    "            if isinstance(_out, tuple):\n",
    "                _out = _out[0]\n",
    "            # accumulate outputs\n",
    "            output = torch.cat((output, _out))\n",
    "            start_idx = end_idx\n",
    "        # Run the head forward on the concatenated features.\n",
    "        return self.head(output)\n",
    "\n",
    "def get_params_groups(model):\n",
    "    regularized = []\n",
    "    not_regularized = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        # we do not regularize biases nor Norm parameters\n",
    "        if name.endswith(\".bias\") or len(param.shape) == 1:\n",
    "            not_regularized.append(param)\n",
    "        else:\n",
    "            regularized.append(param)\n",
    "    return [{'params': regularized}, {'params': not_regularized, 'weight_decay': 0.}]\n",
    "  \n",
    "    \n",
    "def bool_flag(s):\n",
    "    \"\"\"\n",
    "    Parse boolean arguments from the command line.从命令行解析布尔参数。\n",
    "    \"\"\"\n",
    "    FALSY_STRINGS = {\"off\", \"false\", \"0\"}\n",
    "    TRUTHY_STRINGS = {\"on\", \"true\", \"1\"}\n",
    "    if s.lower() in FALSY_STRINGS:\n",
    "        return False\n",
    "    elif s.lower() in TRUTHY_STRINGS:\n",
    "        return True\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(\"invalid value for a boolean flag\")\n",
    "\n",
    "torchvision_archs = sorted(name for name in torchvision_models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(torchvision_models.__dict__[name]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41172a58-da4b-44b8-9771-0ed1efb9a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proj_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Proj_layer, self).__init__()\n",
    "        self.patch_size = 224\n",
    "    #         self.params=nn.Parameter(torch.randn(4, 1))\n",
    "        self.conv_2d = nn.Conv2d(1, 192, kernel_size=(16, 16), stride=(16, 16))\n",
    "    def forward(self, x):\n",
    "        x = self.conv_2d(x)\n",
    "        x = x.transpose(1,3)\n",
    "        x = x.squeeze(2)\n",
    "        x = rearrange(x, 'b c h w -> b (c h) w')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e5508f0-0a02-4251-aeb5-b1feb9bf1e39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_dino(args):\n",
    "    utils.init_distributed_mode(args)\n",
    "    utils.fix_random_seeds(args.seed)\n",
    "    print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "    print(\"\\n\".join(\"%s: %s\" % (k, str(v)) for k, v in sorted(dict(vars(args)).items())))\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # ============ preparing data ... ============ 准备数据\n",
    "#     transform_ADJ = DataAugmentationDINO(\n",
    "#         args.global_crops_scale,\n",
    "#         args.local_crops_scale,\n",
    "#         args.local_crops_number,\n",
    "#     )\n",
    "\n",
    "\n",
    "    global_min, global_max = get_dataset_minmax(dataset_path)\n",
    "    dataset_train = TerrainDataset_BiNorm_Noise_on_g(dataset_path, transform_ADJ, transform_G, global_min, global_max)\n",
    "    print(\"两种归一化的数据集构建完成\")\n",
    "    # 获取bof特征\n",
    "    if load_bof_voc:\n",
    "        bof_voc, bof_stdSlr = torch.load(\"./bof_voc/voc.pth\")\n",
    "        voc_k = bof_voc.shape[0]\n",
    "        print(\"voc_k = \", voc_k, \" (代表sift_bof特征维度为此值，后续的嵌入会沿用这一切分维度)\")\n",
    "        print(\"成功加载bof_voc\")\n",
    "    else:\n",
    "        voc_k = 100\n",
    "        bof_voc, bof_stdSlr = get_bof_voc(dataset_path, using_num = 2000, voc_k = voc_k)\n",
    "        torch.save((bof_voc, bof_stdSlr), \"./bof_voc/voc.pth\")\n",
    "    # 获取高程分位数\n",
    "    _, _, per_li = get_dataset_per(dataset_path, args.num_ele_slice)\n",
    "    print(\"获取数据集高程分位数完成\")\n",
    "    \n",
    "    _, _, per_li_var = get_dataset_var_per(dataset_path, args.num_ele_slice)\n",
    "    print(\"获取数据集高程分位数(var)完成\")\n",
    "#     dataset_train = TerrainDataset(dataset_path, transform = transform)\n",
    "#     dataset_test = TerrainDataset('./Data_geo/深度学习地形数据/test')\n",
    "\n",
    "        \n",
    "\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset_train)\n",
    "#     test_sampler = torch.utils.data.distributed.DistributedSampler(dataset_test)\n",
    "    \n",
    "#     dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=args.batch_size_per_gpu, sampler=train_sampler, num_workers=args.num_workers, pin_memory=True, drop_last=True)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset_train, batch_size=args.batch_size_per_gpu, sampler=train_sampler, num_workers=args.num_workers, pin_memory=True, drop_last=True)\n",
    "#     dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=args.batch_size_per_gpu, sampler=test_sampler, num_workers=args.num_workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "#     print(f\"Data loaded: there are {len(dataset_train),len(dataset_test)} images.\")\n",
    "    print(f\"Data loaded: there are {len(dataset_train)} images.\")\n",
    "\n",
    "    # ============ building student and teacher networks ... ============构建网络\n",
    "    # we changed the name DeiT-S for ViT-S to avoid confusions\n",
    "    args.arch = args.arch.replace(\"deit\", \"vit\")\n",
    "    # if the network is a Vision Transformer (i.e. vit_tiny, vit_small, vit_base)\n",
    "    if args.arch in vits.__dict__.keys():\n",
    "        student = vits.__dict__[args.arch](\n",
    "            patch_size=args.patch_size,\n",
    "            drop_path_rate=args.drop_path_rate,  # stochastic depth\n",
    "        )\n",
    "        teacher = vits.__dict__[args.arch](patch_size=args.patch_size)\n",
    "        embed_dim = student.embed_dim\n",
    "            # 维度更改\n",
    "        PatchEmbed_1ch = Proj_layer()\n",
    "        teacher.patch_embed = PatchEmbed_1ch\n",
    "        PatchEmbed_1ch2 = Proj_layer()\n",
    "        student.patch_embed = PatchEmbed_1ch2\n",
    "    # if the network is a XCiT\n",
    "    elif args.arch in torch.hub.list(\"facebookresearch/xcit:main\"):\n",
    "        student = torch.hub.load('facebookresearch/xcit:main', args.arch,pretrained=False, drop_path_rate=args.drop_path_rate)\n",
    "        teacher = torch.hub.load('facebookresearch/xcit:main', args.arch, pretrained=False)\n",
    "        embed_dim = student.embed_dim\n",
    "    # otherwise, we check if the architecture is in torchvision models\n",
    "    elif args.arch in torchvision_models.__dict__.keys():\n",
    "        student = torchvision_models.__dict__[args.arch]()\n",
    "        teacher = torchvision_models.__dict__[args.arch]()\n",
    "        embed_dim = student.fc.weight.shape[1]\n",
    "    elif args.arch == \"BiNet_NO1\":\n",
    "        student = TBiNet()\n",
    "        teacher = TBiNet()\n",
    "        embed_dim = student.embed_dim\n",
    "    elif args.arch == \"EEnT_tiny_NO1\":\n",
    "        student = EEnT_tiny(num_ele_slice = args.num_ele_slice, in_chans=1)\n",
    "        teacher = EEnT_tiny(num_ele_slice = args.num_ele_slice, in_chans=1)\n",
    "        embed_dim = student.embed_dim\n",
    "    elif args.arch == \"EEnT_tiny_NO2\":\n",
    "        student = EEnT_NO2_tiny(num_ele_slice = args.num_ele_slice, per_li = per_li, in_chans=1)\n",
    "        teacher = EEnT_NO2_tiny(num_ele_slice = args.num_ele_slice, per_li = per_li, in_chans=1)\n",
    "        embed_dim = student.embed_dim\n",
    "    elif args.arch == \"BiNet_Res_NO1\":\n",
    "        student = TBiNet_Res(conv_em_dim = 16)\n",
    "        teacher = TBiNet_Res(conv_em_dim = 16)\n",
    "        embed_dim = student.embed_dim\n",
    "    elif args.arch in Py10Bi_model.__dict__.keys():\n",
    "        try:\n",
    "            student = Py10Bi_model.__dict__[args.arch]() # 调用函数\n",
    "            teacher = Py10Bi_model.__dict__[args.arch]()\n",
    "            embed_dim = student.embed_dim\n",
    "        except: # 兼容需要slice和分位数列表的网络\n",
    "            student = Py10Bi_model.__dict__[args.arch](num_ele_slice=args.num_ele_slice, per_li=per_li, per_li_var=per_li_var, \n",
    "                                                       bof_voc=bof_voc, bof_stdSlr=bof_stdSlr, in_chans=1, voc_k = voc_k,\n",
    "                                                      g_min = global_min, g_max = global_max) # 调用函数\n",
    "            teacher = Py10Bi_model.__dict__[args.arch](num_ele_slice=args.num_ele_slice, per_li=per_li, per_li_var=per_li_var,\n",
    "                                                       bof_voc=bof_voc, bof_stdSlr=bof_stdSlr, in_chans=1, voc_k = voc_k,\n",
    "                                                      g_min = global_min, g_max = global_max)\n",
    "            embed_dim = student.embed_dim\n",
    "    else:\n",
    "        print(f\"Unknow architecture: {args.arch}\")\n",
    "        assert 0\n",
    "\n",
    "\n",
    "#     print(teacher)\n",
    "    # multi-crop wrapper handles forward with inputs of different resolutions多裁剪器\n",
    "    student = MultiCropWrapper_Bi(student, DINOHead_Bi(\n",
    "        embed_dim,\n",
    "        args.out_dim,\n",
    "        use_bn=args.use_bn_in_head,\n",
    "        norm_last_layer=args.norm_last_layer,\n",
    "    ))\n",
    "    teacher = MultiCropWrapper_Bi(\n",
    "        teacher,\n",
    "        DINOHead(embed_dim, args.out_dim, args.use_bn_in_head),\n",
    "    )\n",
    "    \n",
    "    # move networks to gpu\n",
    "    student, teacher = student.cuda(), teacher.cuda()\n",
    "    \n",
    "    # synchronize batch norms (if any)同步批处理规范\n",
    "    if utils.has_batchnorms(student):\n",
    "        student = nn.SyncBatchNorm.convert_sync_batchnorm(student)\n",
    "        teacher = nn.SyncBatchNorm.convert_sync_batchnorm(teacher)\n",
    "\n",
    "        # we need DDP wrapper to have synchro batch norms working...我们需要DDP包装有同步批处理规范工作\n",
    "        teacher = nn.parallel.DistributedDataParallel(teacher, device_ids=[args.gpu])\n",
    "        teacher_without_ddp = teacher.module\n",
    "    else:\n",
    "        # teacher_without_ddp and teacher are the same thing\n",
    "        teacher_without_ddp = teacher\n",
    "    student = nn.parallel.DistributedDataParallel(student, device_ids=[args.gpu])\n",
    "    \n",
    "    # teacher and student start with the same weights\n",
    "    teacher_without_ddp.load_state_dict(student.module.state_dict())\n",
    "    \n",
    "    # there is no backpropagation through the teacher, so no need for gradients教师网络无需更新梯度\n",
    "    for p in teacher.parameters():\n",
    "        p.requires_grad = False\n",
    "    print(f\"Student and Teacher are built: they are both {args.arch} network.\")\n",
    "\n",
    "    # ============ preparing loss ... ============损失函数\n",
    "    dino_loss = DINOLoss(\n",
    "        args.out_dim,\n",
    "        args.local_crops_number + 2,  # total number of crops = 2 global crops + local_crops_number\n",
    "        args.warmup_teacher_temp,\n",
    "        args.teacher_temp,\n",
    "        args.warmup_teacher_temp_epochs,\n",
    "        args.epochs,\n",
    "    ).cuda()\n",
    "    \n",
    "    # ============ preparing optimizer ... ============优化器\n",
    "    params_groups = utils.get_params_groups(student)\n",
    "    if args.optimizer == \"adamw\":\n",
    "        optimizer = torch.optim.AdamW(params_groups)  # to use with ViTs\n",
    "    elif args.optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(params_groups, lr=0, momentum=0.9)  # lr is set by scheduler\n",
    "    elif args.optimizer == \"lars\":\n",
    "        optimizer = utils.LARS(params_groups)  # to use with convnet and large batches\n",
    "    # for mixed precision training\n",
    "    fp16_scaler = None\n",
    "    if args.use_fp16:\n",
    "        fp16_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    \n",
    "    # ============ init schedulers ... ============初始化调度器\n",
    "    lr_schedule = utils.cosine_scheduler(\n",
    "        args.lr * (args.batch_size_per_gpu * utils.get_world_size()) / 256.,  # linear scaling rule线性化比例缩小\n",
    "        args.min_lr,\n",
    "        args.epochs, len(data_loader),\n",
    "        warmup_epochs=args.warmup_epochs,\n",
    "    )\n",
    "    wd_schedule = utils.cosine_scheduler(\n",
    "        args.weight_decay,\n",
    "        args.weight_decay_end,\n",
    "        args.epochs, len(data_loader),\n",
    "    )\n",
    "    # momentum parameter is increased to 1. during training with a cosine schedule当训练使用余弦调度时，动量参数增加到1\n",
    "    momentum_schedule = utils.cosine_scheduler(args.momentum_teacher, 1,\n",
    "                                               args.epochs, len(data_loader))\n",
    "    print(f\"Loss, optimizer and schedulers ready.\")\n",
    "\n",
    "    # ============ optionally resume training ... ============可选择的恢复训练\n",
    "    to_restore = {\"epoch\": 0}\n",
    "    utils.restart_from_checkpoint(\n",
    "        os.path.join(args.output_dir, \"checkpoint.pth\"),\n",
    "        run_variables=to_restore,\n",
    "        student=student,\n",
    "        teacher=teacher,\n",
    "        optimizer=optimizer,\n",
    "        fp16_scaler=fp16_scaler,\n",
    "        dino_loss=dino_loss,\n",
    "    )\n",
    "    start_epoch = to_restore[\"epoch\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Starting DINO training !\")\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        data_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        # ============ training one epoch of DINO ... ============训练一个周期\n",
    "        train_stats = train_one_epoch(student, teacher, teacher_without_ddp, dino_loss,\n",
    "            data_loader, optimizer, lr_schedule, wd_schedule, momentum_schedule,\n",
    "            epoch, fp16_scaler, args)\n",
    "\n",
    "        # ============ writing logs ... ============显示、存储状态\n",
    "        save_dict = {\n",
    "            'student': student.state_dict(),\n",
    "            'teacher': teacher.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'args': args,\n",
    "            'dino_loss': dino_loss.state_dict(),\n",
    "        }\n",
    "        if fp16_scaler is not None:\n",
    "            save_dict['fp16_scaler'] = fp16_scaler.state_dict()\n",
    "        utils.save_on_master(save_dict, os.path.join(args.output_dir, 'checkpoint.pth'))\n",
    "        if args.saveckp_freq and epoch % args.saveckp_freq == 0:\n",
    "            utils.save_on_master(save_dict, os.path.join(args.output_dir, f'checkpoint{epoch:04}.pth'))\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                     'epoch': epoch}\n",
    "        if utils.is_main_process():\n",
    "            with (Path(args.output_dir) / \"log.txt\").open(\"a\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba909505-e8a2-44a3-bfbb-db24146a82f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(student, teacher, teacher_without_ddp, dino_loss, data_loader,\n",
    "                    optimizer, lr_schedule, wd_schedule, momentum_schedule,epoch,\n",
    "                    fp16_scaler, args):\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Epoch: [{}/{}]'.format(epoch, args.epochs)\n",
    "    log_every_num = (len(data_loader) * 100) // 800\n",
    "    # 每个epoch存储REA模块中的Gating Para\n",
    "    try:\n",
    "        REA_gating_record(epoch, teacher, student)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for it, (images, im_global_norm, _) in enumerate(metric_logger.log_every(data_loader, 10, header)):\n",
    "        # update weight decay and learning rate according to their schedule根据他们的时间表更新权重衰减和学习率\n",
    "        it = len(data_loader) * epoch + it  # global training iteration全局训练迭代\n",
    "        for i, param_group in enumerate(optimizer.param_groups):\n",
    "            param_group[\"lr\"] = lr_schedule[it]\n",
    "            if i == 0:  # 只有第一组是正则化的\n",
    "                param_group[\"weight_decay\"] = wd_schedule[it]\n",
    "#         print(type(images))\n",
    "        # move images to gpu\n",
    "        images = [im.cuda(non_blocking=True) for im in images]\n",
    "        im_global_norm = im_global_norm.cuda()\n",
    "        # teacher and student forward passes + compute dino loss\n",
    "        with torch.cuda.amp.autocast(fp16_scaler is not None):\n",
    "            teacher_output = teacher(images[:2], im_global_norm)  # only the 2 global views pass through the teacher\n",
    "            student_output = student(images, im_global_norm)\n",
    "            loss = dino_loss(student_output, teacher_output, epoch)\n",
    "\n",
    "        if not math.isfinite(loss.item()):\n",
    "            print(\"Loss is {}, stopping training\".format(loss.item()), force=True)\n",
    "            sys.exit(1)\n",
    "\n",
    "        # student update\n",
    "        optimizer.zero_grad()\n",
    "        param_norms = None\n",
    "        if fp16_scaler is None:\n",
    "            loss.backward()\n",
    "            if args.clip_grad:\n",
    "                param_norms = utils.clip_gradients(student, args.clip_grad)\n",
    "            utils.cancel_gradients_last_layer(epoch, student,\n",
    "                                              args.freeze_last_layer)\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            fp16_scaler.scale(loss).backward()\n",
    "            if args.clip_grad:\n",
    "                fp16_scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                param_norms = utils.clip_gradients(student, args.clip_grad)\n",
    "            utils.cancel_gradients_last_layer(epoch, student,\n",
    "                                              args.freeze_last_layer)\n",
    "            fp16_scaler.step(optimizer)\n",
    "            fp16_scaler.update()\n",
    "        \n",
    "\n",
    "        \n",
    "        # EMA update for the teacher使用EMA更新teacher\n",
    "        with torch.no_grad():\n",
    "            m = momentum_schedule[it]  # momentum parameter\n",
    "            for param_q, param_k in zip(student.module.parameters(), teacher_without_ddp.parameters()):\n",
    "                param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)\n",
    "        \n",
    "#         for name, param in student.named_parameters(): # 调试没有梯度的层用\n",
    "#             if param.grad is None:\n",
    "#                 print(name)\n",
    "        \n",
    "        # logging日志\n",
    "        torch.cuda.synchronize()\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "        metric_logger.update(wd=optimizer.param_groups[0][\"weight_decay\"])\n",
    "        if it % log_every_num == 0: # 总记录少于1000个点\n",
    "            writer.add_scalar(\"loss\", loss.item(), it)\n",
    "            writer.add_scalar(\"epoch\", epoch, it)\n",
    "            writer.add_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], it)\n",
    "            writer.add_scalar(\"wd\", optimizer.param_groups[0][\"weight_decay\"], it)\n",
    "    \n",
    "    # gather the stats from all processes收集所有进程的统计信息\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d86a4ac-87a2-4bf2-8496-45ccac351336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DINOLoss(nn.Module):\n",
    "    def __init__(self, out_dim, ncrops, warmup_teacher_temp, teacher_temp,\n",
    "                 warmup_teacher_temp_epochs, nepochs, student_temp=0.1,\n",
    "                 center_momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.student_temp = student_temp\n",
    "        self.center_momentum = center_momentum\n",
    "        self.ncrops = ncrops\n",
    "        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n",
    "        # 我们对教师温度进行了预热，因为过高的温度会使培训在开始时不稳定\n",
    "        self.teacher_temp_schedule = np.concatenate((\n",
    "            np.linspace(warmup_teacher_temp,\n",
    "                        teacher_temp, warmup_teacher_temp_epochs),\n",
    "            np.ones(nepochs - warmup_teacher_temp_epochs) * teacher_temp\n",
    "        ))\n",
    "\n",
    "    def forward(self, student_output, teacher_output, epoch):\n",
    "        \"\"\"\n",
    "        Cross-entropy between softmax outputs of the teacher and student networks.\n",
    "        \"\"\"\n",
    "        student_out = student_output / self.student_temp\n",
    "        student_out = student_out.chunk(self.ncrops)\n",
    "\n",
    "        # teacher centering and sharpening\n",
    "        temp = self.teacher_temp_schedule[epoch]\n",
    "        teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1)\n",
    "        teacher_out = teacher_out.detach().chunk(2)\n",
    "\n",
    "        total_loss = 0\n",
    "        n_loss_terms = 0\n",
    "        for iq, q in enumerate(teacher_out):\n",
    "            for v in range(len(student_out)):\n",
    "                if v == iq:\n",
    "                    # we skip cases where student and teacher operate on the same view我们跳过了学生和老师在同一个视图上操作的情况\n",
    "                    continue\n",
    "                loss = torch.sum(-q * F.log_softmax(student_out[v], dim=-1), dim=-1)\n",
    "                total_loss += loss.mean()\n",
    "                n_loss_terms += 1\n",
    "        total_loss /= n_loss_terms\n",
    "        self.update_center(teacher_output)\n",
    "        return total_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_center(self, teacher_output):\n",
    "        \"\"\"\n",
    "        Update center used for teacher output.\n",
    "        \"\"\"\n",
    "        batch_center = torch.sum(teacher_output, dim=0, keepdim=True)\n",
    "        dist.all_reduce(batch_center)\n",
    "        batch_center = batch_center / (len(teacher_output) * dist.get_world_size())\n",
    "\n",
    "        # ema update\n",
    "        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e30ca5c9-0e14-40f4-9a03-86503d20c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('/root/tf-logs') # init write obj 初始化tensorboard的记录\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912022fb-3cf2-41c8-b686-51234c313dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run the code on one GPU.\n",
      "| distributed init (rank 0): tcp://localhost:10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /root)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git:\n",
      "  sha: N/A, status: clean, branch: N/A\n",
      "\n",
      "arch: GRET_ELE_DB_NO2_bi\n",
      "batch_size_per_gpu: 45\n",
      "clip_grad: 3.0\n",
      "data_path: /path/to/imagenet/train/\n",
      "dist_url: tcp://localhost:10001\n",
      "drop_path_rate: 0.1\n",
      "epochs: 100\n",
      "freeze_last_layer: 1\n",
      "global_crops_scale: (0.4, 1.0)\n",
      "gpu: 0\n",
      "local_crops_number: 8\n",
      "local_crops_scale: (0.05, 0.4)\n",
      "local_rank: 0\n",
      "lr: 0.0005\n",
      "min_lr: 1e-06\n",
      "momentum_teacher: 0.996\n",
      "norm_last_layer: True\n",
      "num_ele_slice: 20000\n",
      "num_workers: 1\n",
      "optimizer: adamw\n",
      "out_dim: 65536\n",
      "output_dir: .\n",
      "patch_size: 16\n",
      "rank: 0\n",
      "saveckp_freq: 20\n",
      "seed: 0\n",
      "teacher_temp: 0.04\n",
      "use_bn_in_head: False\n",
      "use_fp16: True\n",
      "warmup_epochs: 10\n",
      "warmup_teacher_temp: 0.04\n",
      "warmup_teacher_temp_epochs: 0\n",
      "weight_decay: 0.04\n",
      "weight_decay_end: 0.4\n",
      "world_size: 1\n",
      "获取数据集最大最小值……\n",
      "数据集最大最小值为(-9655.287, 5201.252)\n",
      "两种归一化的数据集构建完成\n",
      "voc_k =  100  (代表sift_bof特征维度为此值，后续的嵌入会沿用这一切分维度)\n",
      "成功加载bof_voc\n",
      "获取数据集分位数……\n",
      "数据集最大最小值为(-9655.287, 5201.252)\n",
      "获取数据集高程分位数完成\n",
      "获取数据集分位数……(VAR)\n",
      "数据集最大最小值为(-9655.287, 5201.252)\n",
      "获取数据集高程分位数(var)完成\n",
      "Data loaded: there are 8363 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_xcit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student and Teacher are built: they are both GRET_ELE_DB_NO2_bi network.\n",
      "Loss, optimizer and schedulers ready.\n",
      "Found checkpoint at ./checkpoint.pth\n",
      "=> loaded 'student' from checkpoint './checkpoint.pth' with msg <All keys matched successfully>\n",
      "=> loaded 'teacher' from checkpoint './checkpoint.pth' with msg <All keys matched successfully>\n",
      "=> loaded 'optimizer' from checkpoint: './checkpoint.pth'\n",
      "=> loaded 'fp16_scaler' from checkpoint: './checkpoint.pth'\n",
      "=> loaded 'dino_loss' from checkpoint './checkpoint.pth' with msg <All keys matched successfully>\n",
      "Starting DINO training !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:3454: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2/100]  [  0/185]  eta: 0:22:46  loss: 10.515423 (10.515423)  lr: 0.000018 (0.000018)  wd: 0.040355 (0.040355)  time: 7.384190  data: 4.497261  max mem: 19093\n",
      "Epoch: [2/100]  [ 10/185]  eta: 0:08:55  loss: 10.431251 (10.435861)  lr: 0.000018 (0.000018)  wd: 0.040365 (0.040365)  time: 3.061275  data: 1.626753  max mem: 19201\n",
      "Epoch: [2/100]  [ 20/185]  eta: 0:07:49  loss: 10.426844 (10.427522)  lr: 0.000018 (0.000018)  wd: 0.040375 (0.040375)  time: 2.621343  data: 1.347811  max mem: 19201\n",
      "Epoch: [2/100]  [ 30/185]  eta: 0:07:17  loss: 10.397007 (10.412457)  lr: 0.000019 (0.000018)  wd: 0.040395 (0.040385)  time: 2.691120  data: 1.430761  max mem: 19201\n",
      "Epoch: [2/100]  [ 40/185]  eta: 0:06:44  loss: 10.374393 (10.402686)  lr: 0.000019 (0.000019)  wd: 0.040415 (0.040395)  time: 2.730824  data: 1.420173  max mem: 19201\n",
      "Epoch: [2/100]  [ 50/185]  eta: 0:06:14  loss: 10.349962 (10.390920)  lr: 0.000019 (0.000019)  wd: 0.040436 (0.040405)  time: 2.690662  data: 1.378794  max mem: 19201\n",
      "Epoch: [2/100]  [ 60/185]  eta: 0:05:44  loss: 10.323715 (10.373547)  lr: 0.000020 (0.000019)  wd: 0.040458 (0.040416)  time: 2.679460  data: 1.413514  max mem: 19201\n",
      "Epoch: [2/100]  [ 70/185]  eta: 0:05:14  loss: 10.276274 (10.358041)  lr: 0.000020 (0.000019)  wd: 0.040480 (0.040427)  time: 2.657852  data: 1.393167  max mem: 19201\n",
      "Epoch: [2/100]  [ 80/185]  eta: 0:04:45  loss: 10.233417 (10.341080)  lr: 0.000021 (0.000019)  wd: 0.040502 (0.040438)  time: 2.605010  data: 1.341138  max mem: 19201\n",
      "Epoch: [2/100]  [ 90/185]  eta: 0:04:16  loss: 10.210671 (10.324904)  lr: 0.000021 (0.000020)  wd: 0.040525 (0.040449)  time: 2.562867  data: 1.297678  max mem: 19201\n",
      "Epoch: [2/100]  [100/185]  eta: 0:03:48  loss: 10.173612 (10.308630)  lr: 0.000022 (0.000020)  wd: 0.040549 (0.040460)  time: 2.581401  data: 1.291710  max mem: 19201\n",
      "Epoch: [2/100]  [110/185]  eta: 0:03:21  loss: 10.134296 (10.291116)  lr: 0.000022 (0.000020)  wd: 0.040573 (0.040471)  time: 2.621438  data: 1.333973  max mem: 19201\n",
      "Epoch: [2/100]  [120/185]  eta: 0:02:55  loss: 10.098308 (10.274643)  lr: 0.000023 (0.000020)  wd: 0.040598 (0.040483)  time: 2.714225  data: 1.431636  max mem: 19201\n",
      "Epoch: [2/100]  [130/185]  eta: 0:02:28  loss: 10.056262 (10.255770)  lr: 0.000023 (0.000021)  wd: 0.040623 (0.040495)  time: 2.732764  data: 1.443199  max mem: 19201\n",
      "Epoch: [2/100]  [140/185]  eta: 0:02:01  loss: 9.996581 (10.235695)  lr: 0.000024 (0.000021)  wd: 0.040648 (0.040507)  time: 2.680350  data: 1.410051  max mem: 19201\n",
      "Epoch: [2/100]  [150/185]  eta: 0:01:33  loss: 9.941112 (10.215054)  lr: 0.000024 (0.000021)  wd: 0.040675 (0.040519)  time: 2.629422  data: 1.364224  max mem: 19201\n",
      "Epoch: [2/100]  [160/185]  eta: 0:01:06  loss: 9.909156 (10.193459)  lr: 0.000025 (0.000021)  wd: 0.040701 (0.040531)  time: 2.565572  data: 1.301111  max mem: 19201\n",
      "Epoch: [2/100]  [170/185]  eta: 0:00:40  loss: 9.830337 (10.171457)  lr: 0.000025 (0.000022)  wd: 0.040729 (0.040543)  time: 2.556510  data: 1.291953  max mem: 19201\n",
      "Epoch: [2/100]  [180/185]  eta: 0:00:13  loss: 9.767283 (10.147634)  lr: 0.000026 (0.000022)  wd: 0.040756 (0.040556)  time: 2.600076  data: 1.334588  max mem: 19201\n",
      "Epoch: [2/100]  [184/185]  eta: 0:00:02  loss: 9.752560 (10.137561)  lr: 0.000026 (0.000022)  wd: 0.040768 (0.040561)  time: 2.597308  data: 1.332338  max mem: 19201\n",
      "Epoch: [2/100] Total time: 0:08:13 (2.666601 s / it)\n",
      "Averaged stats: loss: 9.752560 (10.137561)  lr: 0.000026 (0.000022)  wd: 0.040768 (0.040561)\n",
      "Epoch: [3/100]  [  0/185]  eta: 0:15:46  loss: 9.617512 (9.617512)  lr: 0.000026 (0.000026)  wd: 0.040799 (0.040799)  time: 5.118300  data: 3.640902  max mem: 19201\n",
      "Epoch: [3/100]  [ 10/185]  eta: 0:08:16  loss: 9.573149 (9.579525)  lr: 0.000027 (0.000027)  wd: 0.040813 (0.040813)  time: 2.837676  data: 1.526480  max mem: 19201\n",
      "Epoch: [3/100]  [ 20/185]  eta: 0:07:31  loss: 9.551835 (9.559225)  lr: 0.000027 (0.000027)  wd: 0.040828 (0.040828)  time: 2.615413  data: 1.321474  max mem: 19201\n",
      "Epoch: [3/100]  [ 30/185]  eta: 0:06:58  loss: 9.516995 (9.519838)  lr: 0.000027 (0.000027)  wd: 0.040857 (0.040843)  time: 2.628753  data: 1.339176  max mem: 19201\n",
      "Epoch: [3/100]  [ 40/185]  eta: 0:06:28  loss: 9.349900 (9.463139)  lr: 0.000028 (0.000027)  wd: 0.040887 (0.040858)  time: 2.625174  data: 1.293794  max mem: 19201\n",
      "Epoch: [3/100]  [ 50/185]  eta: 0:05:59  loss: 9.251523 (9.417191)  lr: 0.000028 (0.000028)  wd: 0.040918 (0.040873)  time: 2.605781  data: 1.284095  max mem: 19201\n",
      "Epoch: [3/100]  [ 60/185]  eta: 0:05:31  loss: 9.037869 (9.342962)  lr: 0.000029 (0.000028)  wd: 0.040949 (0.040888)  time: 2.583015  data: 1.317657  max mem: 19201\n",
      "Epoch: [3/100]  [ 70/185]  eta: 0:05:04  loss: 8.977399 (9.281381)  lr: 0.000029 (0.000028)  wd: 0.040981 (0.040904)  time: 2.592700  data: 1.328029  max mem: 19201\n",
      "Epoch: [3/100]  [ 80/185]  eta: 0:04:36  loss: 8.823917 (9.212391)  lr: 0.000030 (0.000028)  wd: 0.041013 (0.040919)  time: 2.582507  data: 1.316857  max mem: 19201\n",
      "Epoch: [3/100]  [ 90/185]  eta: 0:04:09  loss: 8.623671 (9.144049)  lr: 0.000030 (0.000029)  wd: 0.041046 (0.040935)  time: 2.549757  data: 1.285254  max mem: 19201\n",
      "Epoch: [3/100]  [100/185]  eta: 0:03:43  loss: 8.549405 (9.076632)  lr: 0.000031 (0.000029)  wd: 0.041079 (0.040951)  time: 2.592517  data: 1.318561  max mem: 19201\n",
      "Epoch: [3/100]  [110/185]  eta: 0:03:16  loss: 8.347759 (8.997092)  lr: 0.000031 (0.000029)  wd: 0.041112 (0.040968)  time: 2.601378  data: 1.273628  max mem: 19201\n",
      "Epoch: [3/100]  [120/185]  eta: 0:02:50  loss: 8.055387 (8.913386)  lr: 0.000032 (0.000029)  wd: 0.041147 (0.040984)  time: 2.594482  data: 1.260568  max mem: 19201\n",
      "Epoch: [3/100]  [130/185]  eta: 0:02:23  loss: 7.840485 (8.823405)  lr: 0.000032 (0.000029)  wd: 0.041181 (0.041000)  time: 2.593530  data: 1.312675  max mem: 19201\n",
      "Epoch: [3/100]  [140/185]  eta: 0:01:57  loss: 7.695073 (8.737174)  lr: 0.000033 (0.000030)  wd: 0.041216 (0.041017)  time: 2.612710  data: 1.348294  max mem: 19201\n",
      "Epoch: [3/100]  [150/185]  eta: 0:01:31  loss: 7.602359 (8.655329)  lr: 0.000033 (0.000030)  wd: 0.041252 (0.041034)  time: 2.673355  data: 1.408811  max mem: 19201\n",
      "Epoch: [3/100]  [160/185]  eta: 0:01:05  loss: 7.348528 (8.563188)  lr: 0.000034 (0.000030)  wd: 0.041288 (0.041051)  time: 2.639849  data: 1.375670  max mem: 19201\n",
      "Epoch: [3/100]  [170/185]  eta: 0:00:39  loss: 7.056192 (8.469712)  lr: 0.000034 (0.000030)  wd: 0.041325 (0.041068)  time: 2.628149  data: 1.336576  max mem: 19201\n",
      "Epoch: [3/100]  [180/185]  eta: 0:00:13  loss: 6.891212 (8.375646)  lr: 0.000034 (0.000031)  wd: 0.041362 (0.041086)  time: 2.644966  data: 1.348765  max mem: 19201\n",
      "Epoch: [3/100]  [184/185]  eta: 0:00:02  loss: 6.828336 (8.324763)  lr: 0.000035 (0.000031)  wd: 0.041378 (0.041093)  time: 2.677346  data: 1.368620  max mem: 19201\n",
      "Epoch: [3/100] Total time: 0:08:05 (2.626848 s / it)\n",
      "Averaged stats: loss: 6.828336 (8.324763)  lr: 0.000035 (0.000031)  wd: 0.041378 (0.041093)\n",
      "Epoch: [4/100]  [  0/185]  eta: 0:15:59  loss: 6.528193 (6.528193)  lr: 0.000035 (0.000035)  wd: 0.041419 (0.041419)  time: 5.188647  data: 3.689503  max mem: 19201\n",
      "Epoch: [4/100]  [ 10/185]  eta: 0:08:30  loss: 6.359832 (6.459506)  lr: 0.000035 (0.000035)  wd: 0.041439 (0.041439)  time: 2.915753  data: 1.613921  max mem: 19201\n",
      "Epoch: [4/100]  [ 20/185]  eta: 0:07:36  loss: 6.300505 (6.218067)  lr: 0.000036 (0.000036)  wd: 0.041458 (0.041458)  time: 2.648064  data: 1.374964  max mem: 19201\n",
      "Epoch: [4/100]  [ 30/185]  eta: 0:07:02  loss: 5.887836 (6.055127)  lr: 0.000036 (0.000036)  wd: 0.041497 (0.041478)  time: 2.616996  data: 1.352372  max mem: 19201\n",
      "Epoch: [4/100]  [ 40/185]  eta: 0:06:33  loss: 5.677764 (5.910825)  lr: 0.000037 (0.000036)  wd: 0.041537 (0.041497)  time: 2.649273  data: 1.382465  max mem: 19201\n",
      "Epoch: [4/100]  [ 50/185]  eta: 0:06:06  loss: 5.230112 (5.769152)  lr: 0.000037 (0.000036)  wd: 0.041577 (0.041517)  time: 2.696340  data: 1.429195  max mem: 19201\n",
      "Epoch: [4/100]  [ 60/185]  eta: 0:05:37  loss: 5.039343 (5.640225)  lr: 0.000038 (0.000037)  wd: 0.041617 (0.041537)  time: 2.676961  data: 1.412438  max mem: 19201\n",
      "Epoch: [4/100]  [ 70/185]  eta: 0:05:11  loss: 4.927348 (5.521336)  lr: 0.000038 (0.000037)  wd: 0.041658 (0.041558)  time: 2.685065  data: 1.421127  max mem: 19201\n",
      "Epoch: [4/100]  [ 80/185]  eta: 0:04:42  loss: 4.611364 (5.391686)  lr: 0.000039 (0.000037)  wd: 0.041700 (0.041578)  time: 2.668526  data: 1.404448  max mem: 19201\n",
      "Epoch: [4/100]  [ 90/185]  eta: 0:04:14  loss: 4.464037 (5.291215)  lr: 0.000039 (0.000037)  wd: 0.041742 (0.041599)  time: 2.603891  data: 1.341109  max mem: 19201\n",
      "Epoch: [4/100]  [100/185]  eta: 0:03:47  loss: 4.259910 (5.171700)  lr: 0.000039 (0.000038)  wd: 0.041785 (0.041620)  time: 2.615593  data: 1.352786  max mem: 19201\n",
      "Epoch: [4/100]  [110/185]  eta: 0:03:20  loss: 3.968651 (5.057702)  lr: 0.000040 (0.000038)  wd: 0.041828 (0.041640)  time: 2.646694  data: 1.363139  max mem: 19201\n",
      "Epoch: [4/100]  [120/185]  eta: 0:02:53  loss: 3.916328 (4.959144)  lr: 0.000040 (0.000038)  wd: 0.041872 (0.041662)  time: 2.617945  data: 1.315764  max mem: 19201\n",
      "Epoch: [4/100]  [130/185]  eta: 0:02:26  loss: 3.809759 (4.863067)  lr: 0.000041 (0.000038)  wd: 0.041916 (0.041683)  time: 2.573801  data: 1.291244  max mem: 19201\n",
      "Epoch: [4/100]  [140/185]  eta: 0:01:59  loss: 3.532864 (4.769111)  lr: 0.000041 (0.000039)  wd: 0.041961 (0.041704)  time: 2.586780  data: 1.322829  max mem: 19201\n",
      "Epoch: [4/100]  [150/185]  eta: 0:01:33  loss: 3.424004 (4.678138)  lr: 0.000042 (0.000039)  wd: 0.042006 (0.041726)  time: 2.652606  data: 1.388104  max mem: 19201\n",
      "Epoch: [4/100]  [160/185]  eta: 0:01:06  loss: 3.288401 (4.591046)  lr: 0.000042 (0.000039)  wd: 0.042052 (0.041748)  time: 2.713652  data: 1.448186  max mem: 19201\n",
      "Epoch: [4/100]  [170/185]  eta: 0:00:39  loss: 3.167443 (4.502141)  lr: 0.000043 (0.000039)  wd: 0.042098 (0.041770)  time: 2.668315  data: 1.402891  max mem: 19201\n",
      "Epoch: [4/100]  [180/185]  eta: 0:00:13  loss: 2.921038 (4.417683)  lr: 0.000043 (0.000039)  wd: 0.042145 (0.041792)  time: 2.657473  data: 1.393117  max mem: 19201\n",
      "Epoch: [4/100]  [184/185]  eta: 0:00:02  loss: 2.920683 (4.387492)  lr: 0.000043 (0.000040)  wd: 0.042164 (0.041801)  time: 2.664793  data: 1.401311  max mem: 19201\n",
      "Epoch: [4/100] Total time: 0:08:12 (2.663613 s / it)\n",
      "Averaged stats: loss: 2.920683 (4.387492)  lr: 0.000043 (0.000040)  wd: 0.042164 (0.041801)\n",
      "Epoch: [5/100]  [  0/185]  eta: 0:15:00  loss: 3.129098 (3.129098)  lr: 0.000044 (0.000044)  wd: 0.042216 (0.042216)  time: 4.868764  data: 3.587669  max mem: 19201\n",
      "Epoch: [5/100]  [ 10/185]  eta: 0:08:24  loss: 3.129098 (3.108251)  lr: 0.000044 (0.000044)  wd: 0.042240 (0.042240)  time: 2.880925  data: 1.617418  max mem: 19201\n",
      "Epoch: [5/100]  [ 20/185]  eta: 0:07:37  loss: 2.882198 (2.874891)  lr: 0.000044 (0.000044)  wd: 0.042264 (0.042264)  time: 2.665583  data: 1.403512  max mem: 19201\n",
      "Epoch: [5/100]  [ 30/185]  eta: 0:07:02  loss: 2.731913 (2.859941)  lr: 0.000045 (0.000045)  wd: 0.042313 (0.042289)  time: 2.641151  data: 1.379389  max mem: 19201\n",
      "Epoch: [5/100]  [ 40/185]  eta: 0:06:31  loss: 2.731913 (2.807227)  lr: 0.000045 (0.000045)  wd: 0.042362 (0.042313)  time: 2.625292  data: 1.353188  max mem: 19201\n",
      "Epoch: [5/100]  [ 50/185]  eta: 0:06:01  loss: 2.722575 (2.801528)  lr: 0.000046 (0.000045)  wd: 0.042411 (0.042338)  time: 2.605687  data: 1.333630  max mem: 19201\n",
      "Epoch: [5/100]  [ 60/185]  eta: 0:05:34  loss: 2.576925 (2.751958)  lr: 0.000046 (0.000045)  wd: 0.042462 (0.042363)  time: 2.621691  data: 1.361957  max mem: 19201\n",
      "Epoch: [5/100]  [ 70/185]  eta: 0:05:07  loss: 2.560102 (2.722313)  lr: 0.000047 (0.000046)  wd: 0.042512 (0.042388)  time: 2.657051  data: 1.395947  max mem: 19201\n",
      "Epoch: [5/100]  [ 80/185]  eta: 0:04:39  loss: 2.480436 (2.682463)  lr: 0.000047 (0.000046)  wd: 0.042563 (0.042413)  time: 2.641392  data: 1.379573  max mem: 19201\n",
      "Epoch: [5/100]  [ 90/185]  eta: 0:04:13  loss: 2.361259 (2.655806)  lr: 0.000048 (0.000046)  wd: 0.042615 (0.042438)  time: 2.639509  data: 1.378686  max mem: 19201\n"
     ]
    }
   ],
   "source": [
    "# %tb ==========主函数===============\n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser('DINO', parents=[get_args_parser()])\n",
    "#     args = parser.parse_args(args = args_self)\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    train_dino(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a1c34-de36-4854-86d4-7f863a650363",
   "metadata": {},
   "source": [
    "# 训练后处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40679ad6-b40c-4c4e-aef9-2e5f531142f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移动网络到存到文件夹，并重命名（自动累加）\n",
    "import shutil\n",
    "save_net_name_unique, netname = get_netpath_unique(\"【\" + args.arch  + \"\" +\"】\")\n",
    "shutil.move(\"./checkpoint.pth\", save_net_name_unique) # 将其移动到应该的位置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9382f4f-2f8c-4770-875c-cf3e7eb1fa9b",
   "metadata": {},
   "source": [
    "# 继续【NB14】BiNet_eval_based_on_knn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701cf84-ddc3-46fc-8fce-6fc2bb2e410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save_net_name_unique\n",
    "# save_net_name_unique = './【checkpoint存档】/20230725【GRET_NO1_bi】DINO_3_01.pth'\n",
    "# # save_net_name_unique = \"./checkpoint.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f500f33-2d66-4b8c-9976-361a5a32cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model_path_outside = save_net_name_unique\n",
    "# net_name_outside = netname\n",
    "arch_outside = args.arch\n",
    "num_ele_slice_outside = args.num_ele_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c677c-b556-4622-a385-69c0dff501c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dir())\n",
    "# # 清除\n",
    "# for var in list(globals().keys()):\n",
    "#     if not var.startswith(\"_\") and not var.endswith(\"_outside\"):\n",
    "#         del globals()[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8938f36a-4152-4aac-b28b-2924e9b28b56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run ./【NB14】BiNet_eval_based_on_knn.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b73a470-c605-4792-8be0-ba6c1b667abb",
   "metadata": {},
   "source": [
    "# 继续【NB15】BiNet_Encode_map2Vec.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605bd6d-1a36-40a3-9743-b4b315d57af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dir())\n",
    "# # 清除\n",
    "# for var in list(globals().keys()):\n",
    "#     if not var.startswith(\"_\") and not var.endswith(\"_outside\"):\n",
    "#         del globals()[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d2fd2-53d8-4fb6-83e4-859d6bcd982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache() #使用memory_allocated前先清空一下cache\n",
    "torch.cuda.memory_allocated()\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc71d5-41ce-451a-ae47-d681843365b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run ./【NB15】BiNet_Encode_map2Vec.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd72696-5b3d-4a1f-8e6e-68be6ea5b99b",
   "metadata": {},
   "source": [
    "# 继续【NB16】BiNet_Matching_Iter.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126fa607-07e7-4d55-962c-5c2d3e9339f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dir())\n",
    "# # 清除\n",
    "# for var in list(globals().keys()):\n",
    "#     if not var.startswith(\"_\") and not var.endswith(\"_outside\"):\n",
    "#         del globals()[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e9b0b-b33b-4d67-98c3-7fe71ce19663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache() #使用memory_allocated前先清空一下cache\n",
    "torch.cuda.memory_allocated()\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "# print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a67c4c-5199-409f-a5a4-c642aad67433",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./【NB16】BiNet_Matching_Iter.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d31787-652b-478d-9356-82e70146a9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8608ee-94a2-4cf9-81ec-846f76fbca96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
